# 一、实验总结

## 1、COPA: Kubernetes的一种组合自动缩放方法
> 《COPA: A Combined Autoscaling Method for Kubernetes》  

### 实验条件  
        
        实验在Kubernetes v1.17集群上进行，Docker版本为19.03.13。集群包含6个node和1个Master。每个Node配置4vCPU和4G内存，Master配置8vCPU和16G内存，Master和Node的CPU为Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz。
### 实验数据  
        该文主要实验数据包括收集到的微服务性能数据、实时工作负载、预期响应时间。

        实验中使用的基准应用程序是用PHP实现的MD5加密算法。云原生微服务通常是细粒度任务，比如图像搜索、图像识别、文档翻译以及视频或音频解码任务这些也可以考虑。

        该文使用的与其他文章的“使用CPU、内存和其他指标的利用率作为伸缩的基础，当利用率达到某个阈值时，就会触发伸缩”不同，他建立了资源供给与高级指标之间的映射关系，即获得不同资源供给下微服务的处理能力(每秒可处理的最大请求数)。收集上述跟踪数据的方法是使用垂直缩放来改变实例资源的供给，并对该资源供给下的实例进行压力测试。

        模拟用户的请求，使用Hey负载生成器[32]向Benchmark应用程序发送HTTP请求。 --Rakyll hey. [Online]. Available: https://github.com/rakyll/hey

        从Wikipedia Access trace中提取了4种不同的工作负载类型，并按比例压缩，分别为gentle, burst, rise, and decline workloads。--G. Urdaneta, G. Pierre, and M. V an Steen, “Wikipedia workload analysis for decentralized hosting,” Computer Networks, vol. 53, no. 11, pp. 1830–1845, 2009.
        

## 2、基于机器学习的Kubernetes边缘集群扩展管理
> 《Machine Learning-Based Scaling Management for Kubernetes Edge Clusters》  
### 实验条件
        在Amazon Web Services (AWS)中搭建的k8s平台，使用了Kubernetes Operations (kops)[43]，这是一个用于在AWS上部署和维护生产级高可用Kubernetes集群的开源工具。我们的集群包含一个主节点和五个工作节点。均为EC2 m5。xlarge实例，即每个实例有4个vcpu和16 GB RAM。--[43] kOps—Kubernetes Operations. Accessed : Jan. 29, 2021. [Online].Available: https://github.com/kubernetes/kops

        用Python实现的缩放引擎，使用Node.js Web应用程序，通过用作服务负载平衡器的HAProxy入口控制器[44]进行访问，使用了Prometheus监控系统[36]。--[44] HAProxy. Accessed : Jan. 29, 2021. [Online]. Available: https://www.haproxy.com/  --[36] Prometheus. [Online]. Available: https://prometheus.io/


### 实验数据
        经常使用到的比较老的可以获得的数据集有NASA-HTTP[5](向位于佛罗里达州的NASA肯尼迪航天中心Web服务器发出的两个月的所有HTTP请求)和WorldCup98[6](向1998年世界杯网站发出的三个月的HTTP请求)。--[5] NASA-HTTP Logs. Accessed : Jan. 29, 2021. [Online]. Available:ftp://ita.ee.lbl.gov/html/contrib/NASA-HTTP .html  --[6] WorldCup98 HTTP Logs. Accessed : Jan. 29, 2021. [Online]. Available:ftp://ita.ee.lbl.gov/html/contrib/WorldCup.html

        由于数据比较老，该文使用的是对大学校园网络中收集的最近一周的匿名网络流量数据进行的模拟。数据集包含每秒到Facebook的流量计数。（这个数据是否是他进行的一个模拟数据，而非真实数据。Facebook的流量数据是无法获取到的。）还生成了一个基于MMPP的流量数据，用于模拟。使用[38]中描述的算法，对Facebook的流量数据进行了MMPP拟合，该合成流量用以MMPP/M/c的模型的输入。 --[38] A. Rajabi and J. W. Wong, “MMPP characterization of Web application traffic,” in Proc. 20th IEEE Int. Symp. Model. Anal. Simulat. Comput.Telecommun. Syst. (MASCOTS), 2012, pp. 107–114.

### 使用的队列模型
        基于队列模型:一些队列模型通过假设无内存到达过程和自适应的服务器数量来描述扩展系统。Kaboudan[13]使用基于阈值的扩展策略，提出了一个具有动态服务器数量的离散队列模型。他们进行了模拟，并将他们的模型与M/M/c队列的行为进行了比较。--[13] M. A. Kaboudan, “A dynamic-server queuing simulation,” Comput. Oper . Res., vol. 25, no. 6, pp. 431–439, 1998.


        Mazalov和Gurtov[14]提出了一个服务器按需数量的排队模型，该模型依赖于队列的长度，他们假设了一个具有指定速率的泊松到达过程。Jia等[15]引入了一种队列模型，该模型使用基于阈值的策略来更好地描述工业生产过程。该模型假设一个马可夫调制泊松过程(MMPP)作为到达点，并根据队列长度从两个预定义值中选择服务器数量。在所有这些引用的论文中，我们看到马尔可夫的指数到达时间假设是一个限制因素。--[14] V . V . Mazalov and A. Gurtov, “Queuing system with on-demand number of servers,” Mathematica Applicanda, vol. 40, no. 2, pp. 1–12, 2012.  --[15] Y .-H. Jia, L.-X. Tang, Z. G. Zhang, and X.-F. Chen, “MMPP/M/C queue with congestion-based staffing policy and applications in operations of steel industry,” Springer J. Iron Steel Res. Int., vol. 26, no. 7,pp. 659–668, 2018.

        但是该文中作者还是使用了同COPA文章相同的MMPP/M/c模型。
**（可以考虑使用不同的队列模型来进行实验），文中对成本描述只有一句“The Sum of Pod minutes value shows how much it would cost to run the service until a given moment on the x axis, if the 1-minute usage of a Pod cost $1.”**


`在大多数情况下，由于缺乏实现代码，或关于精确算法的详细解释，或其中的参数设置，对其性能的评估是困难的。此外，这些方法中有几种只缩放虚拟机，或者只垂直缩放容器，这使得定量比较具有挑战性。因此，我们选择了两种水平缩放容器的方法，并在Python中实现了它们的算法，然后使用我们的Facebook跟踪运行它们的模拟。为了涵盖根本不同的解决方案，我们选择了基于阈值的方法（Elascale[12]）和基于强化学习的方法[21]）。模拟结果如图所示`

## 3、用于容器化应用的预测性混合自动缩放
> 《Predictive Hybrid Autoscaling for Containerized Applications》
### 实验条件
        与上一篇文献相同，使用Prometheus监控。除了默认指标外，通过额外的扩展，Prometheus还可以获取从入口控制器公开的服务性能指标。使用Locust作为工作负载生成器来生成HTTP请求。

        实验使用Kubernetes的1.23版本。作者在三台裸金属服务器（Bare Metal Server）上部署带有一个主节点和两个工作节点的集群，每台服务器都有128核CPU和64GB内存作为计算资源。

` 裸金属服务器：市场上常见的云服务器，比如说阿里云的ECS、腾讯云的CVM、西部数码的云服务器等都是由物理主机虚拟化得来的，所以根据阿里云和腾讯云的官网文档，是不支持二次虚拟化的。仅有裸金属服务器（阿里云）和黑石物理服务器（腾讯云）是支持第三方虚拟化软件的，针对于有特殊场景，普通的云服务器已经没办法解决其需求的使用场景`

### 实验数据
        同上一篇文献讲的过时数据集相同，使用的是Wikipedia Access trace 和 FIFA World Cup 98 trace的数据。
        
        性能模型（the performance model）的初始训练数据是通过利用[30]中提出的方法生成的，该方法执行跟踪驱动模拟来收集数据集。--[30] M. Abdullah, W. Iqbal, A. Erradi, and F. Bukhari, ‘‘Learning predictive autoscaling policies for cloud-hosted microservices using trace-driven modeling,’’ in Proc. IEEE Int. Conf. Cloud Comput. Technol. Sci. (CloudCom), Dec. 2019, pp. 119–126.

        使用Locust[31]部署应用程序并生成合成工作负载。--[31] Locust. A Modern Load Testing Framework. Accessed: Apr. 20, 2020.[Online]. Available: https://locust.io/

        对性能指标的跟踪记录（The performance trace）包括请求率、响应时间、pod的数量、部署的每个pod的资源以及平均pod利用率。

        该文使用了多种机器学习方法来训练性能模型，包括线性回归(LR)、随机决策森林(RDF)回归、支持向量回归(SVR)、XGBoost (XGB)回归、多层感知器回归(MLP)和决策树回归(DTR)，最终实验数据表名DTR最优，所以选择的使用DTR作为性能模型的学习方法。

<br/>

# 二、idea和工作安排

        1、可以考虑的一个场景是混合编排应用到边缘计算k8s，使用KubeEdge，后面找一些该方向论文，看有没有做过边缘计算场景的。
        2、将成本感知（节约资源成本）加入到混合编排中，可以将COPA的作为对比实验复现。HPA+一个文献中就混合了四个预测模型的实验。
        3、现在主要在回顾之前的文献进行实验条件和实验数据的整理。接下来多找一些顶刊或者一二区的混合编排的期刊文献（但是感觉不太好找）中有没有比较好的算法可以借鉴。
        4、DRL可以参考的预测指标有做混合编排的CPU、内存、其他指标的利用率、微服务性能数据、实时工作负载、预期响应时间等。
        5、如果能有多个启发式算法或者RL算法能实现，可以考虑进行多种算法的混合编排，在规定时效内进行择优选择合适算法进行缩放。